# Day0: 生成业务query列表，比对效果。

## STEP1:生成query列表，对比openai的RAG和自建RAG的效果
### query 类型(后续待扩充):
    - 根据规则进行结果判定
    - 规则索引
    - 数据卡索引
    - 根据数据卡进行规则判定

### 上下文的数据

该代码库中 ../DATAUPLOAD/DATASET/ 三个文件

### 自建RAG代码

本代码库,参考readme文档

最终结果（需要权限，直接联系我)
https://www.notion.so/210bdec9833580029d2febe29c22358a?v=210bdec983358063abe6000c1b7aaa55

上文中包括了openai和本地RAG的回答结果对比，从问题类型上，"根据数据卡进行规则判定"两边处理的都不是很好

## STEP2: 让openai比较答案，并提供本代码库地址，让openai分析代码后给出建议:

- 对话使用的 openai模型: O4-mini-high

- PROMPT: 

这个CSV里是我用自建RAG做的回答（第三列）和你的回答（第二列），这是我的RAG系统代码库：https://github.com/francisk/rag-wh40k。请你比较两个系统的回答，结合我提供的代码，指出我自建系统的问题所在。你可以从内容切片，query处理, 答案生成等角度来评价

- RESPONSE:

1. 内容切片（Chunking）问题
字符切片而非Token切片
你的 DATAUPLOD/upsert.py 默认使用 --chunk-size 1500 和 --chunk-overlap 150，很可能是以字符数为单位拆分，导致：

一些规则条款被强行截断在半句话中，缺少上下文（如“史诗伟业计谋”“英勇介入”等完全条目未能整体检索到）。

ChatGPT 的答案中能够完整列出所有计谋（例如“指挥重掷”“反攻”“史诗挑战”等10+项），而RAG系统常常只返回“未能检索到”或漏条目。未做语义边界处理.没有基于句号、标题或Markdown标题进行二次切片，导致相关规则被拆到不同chunk，重叠也不足以覆盖跨chunk引用的条款。

建议：

改用基于Token的切片（结合 tiktoken 计算），以控制每片最大Token数，并在句末或标题处优先断句。

对“特例/覆盖规则”加预处理标签，将此类重要条款保留在同一chunk或单独标记，以保证后续检索优先命中。

2. 查询处理（Query Processing）不足
缺乏同义/别名映射
例如查询“艾达灵族”（Aeldari）、“Asurman（阿苏曼）”等中英文混用时，系统并未做别名归一化，导致检索不到对应chunk。

简单扩展策略
query_expander.py 可能只进行了基本的英文分词或中文分句，没有针对Warhammer特定术语做扩展（如“深入打击”→“deep strike”）。

标点和数字处理不当
像”12寸“、“6+”这些带符号的表达在预处理时可能被误删或拆分，导致检索时得不到匹配。

建议：

为常见单位／派系／武器建立词典（alias map），在expander里做替换或额外检索。

保留专有名词、数字+单位的原文，避免过度清洗标点。

增加拼写纠错或相似度补齐策略（例如“YNNARI Wyches”↔“死神军巫灵”）。

3. 检索策略（Vector Search）问题
Top K过小
目前 vector_search.py 可能只返回前3–5条相似度最高chunk，许多细节条款落选。

无混合检索（Hybrid Search）
纯向量检索容易遗漏完全匹配的关键句。ChatGPT答案则是通过多chunk组合，确保覆盖面。

未使用metadata排名
chunk中如果带有 chunk_type、faction、source_file 等metadata，可用来提升同派系／同章节的优先级，但当前并未见到对这些字段的二次排序或过滤。

建议：

增大Top K或动态调整（节数多时更多，少时更少）。

引入BM25或基于关键词的精准检索，与向量检索结果合并重排。

在索引时保存并利用chunk_type/coverage等标签，检索后按需求优先注重“特例”“核心”标签。

4. 答案生成（LLM Prompt & Synthesis）缺陷
模板化程度过低
自建系统直接把检索到的chunk扔给LLM，LLM无法判断哪些是关键事实，最后只输出“根据提供的上下文未能找到答案”或简单改写检索结果。

缺乏明确指令
没有在prompt中要求“列出所有核心计谋并标注来源”、“给出具体数值并带单位”“一步一步说明推理过程”。

无Citation指导
自建系统没有在prompt里告诉LLM“请在每个要点末尾插入``格式的引用”，因此即使检索到正确条款，也不会自动生成引用。

建议：

设计更严格的Prompt模板：

明确列出答案格式（用逗号分隔、一行一句）。

要求“对于列表题，务必列出所有条目且带上来源标记”。

对数值题，指明“请给出数值及单位，并说明计算过程”。

在prompt前拼接“以下已检索到的规则段落，请结合它们给出直接回答，并在每条后面标注来源引用”。

5. Citation 与可维护性
缺乏来源回溯
ChatGPT 输出中每项都有精确的 ``，而RAG系统输出完全没有。

日志记录不详细
虽有 app.log，但未见对检索到哪些chunk、相似度、最终prompt拼接内容的记录，不利于后续调试与优化。

建议：

在Vector Search与生成之间，打印或保存“检索到的chunks ID + 相似度”供审核。

将每次生成的LLM输入Prompt和输出Answer保留到日志（含引用映射），以便可追溯。

总结
自建RAG系统目前的短板主要集中在：

粗糙的内容切片（字符切分+少量重叠）

薄弱的Query处理（无同义映射+标点/数字丢失）

单一的检索策略（过少Top K+无混合检索+未利用metadata）

不完善的生成Prompt（未指定格式/引用规则）

缺乏Citation与日志追踪

改善思路：

采用基于Token的语义切片并加“特例”“覆盖”标签；

在Query Expander中做术语归一与多语言支持；

引入Hybrid Search与动态Top K；

设计严谨的LLM生成模板，强制输出列表、数值、步骤和引用；

增强日志与引用管理，提升可维护性与可审计性。

这样才能在精度、覆盖率和可解释性上拉近与你看到的ChatGPT输出的差距。

## STEP3：指出openai自己回答中的错误，让openai自己分析原因:

- 对话使用的 openai模型: O4-mini-high
- PROMPT:

很好，我也来指出你的系统里一些问题，当我问:"一台织夜坦克的主炮射击一个联合单位，这个联合单位由一名大先知，4人规模的战巫议会，10人风暴守护者组成。织夜坦克应该投掷多少个命中骰子"这个问题时，你的分析过程中缺少了分析织夜坦克武器属性的环节，导致你缺少了计算爆炸武器效果的过程。请你从你的系统设计角度分析产生这个遗漏的原因和后续的改进


- RESPONSE:

从系统设计角度看，这次遗漏主要源于两个方面的不足：

检索与切片层面缺失关键规则段落

我们把战车武器属性和“爆炸武器效果”规则切分成了不同的 chunk，且 overlap 太小，导致在检索主炮 Profile 时，没能同时拿到“爆炸武器”那段规则。

改进：在切片时对“武器特性”与“通用武器规则”（如爆炸、火力加值）做特例合并标签（chunk_type=weapon_profile+weapon_rule），保证它们落在同一或紧连的片段里。

答案合成（Answer Synthesis）阶段缺乏领域推理流程

目前的 Prompt 直接让 LLM “根据检索到的上下文回答”，却没强调“先提取武器Profile，再应用爆炸武器额外命中骰规则，再合并目标单位规模”。因此即便检索到了所有相关文本，LLM 也不会主动调用“第二步计算”链式推理。

改进：在生成 Prompt 里加入明确的多步指令，例如：

1. 从“织夜者坦克 – 末日编织者”Profile中读取基础命中骰数量。
2. 查找“爆炸武器”规则，计算额外命中骰或额外效应。
3. 根据目标单位的模型数量，合并所有命中骰并给出总数。
或者拆成两次调用：一次检索与提取武器属性，二次基于属性调用推理模块计算数值。

总结改进方案
更智能的 Chunk Tagging：把同一武器 Profile 与其通用规则（爆炸、贯穿、伤害加成等）贴标签，让检索时一起命中。

强化 Prompt 的 Chain-of-Thought：让 LLM 分步骤执行“提取→应用规则→汇总”流程，降低遗漏复杂规则的概率。

引入小型规则引擎：对核心机械化计算（如命中骰总数、爆炸伤害判定）抽象成可配置的脚本或函数，在最终回答前调用，而不是全靠 LLM 自行推理。

这样才能确保每次遇到需要“先查Profile再算数值”的问题时，都能完整触发所有必要环节，不再遗漏“爆炸武器效果”这类关键步骤。

## STEP4: 形成改进列表:
如本文件夹下的"RAG改进列表.MD"



